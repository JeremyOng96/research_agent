{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "043c9bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jeremyong/Desktop/research_agent/classification\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import aisuite as ai\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../classification/\")\n",
    "print(path)\n",
    "sys.path.insert(0, str(path))\n",
    "from improvements.improvement_method import ImprovementMethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36beb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerationAgent:\n",
    "    def __init__(self, model_name: str, file_type: str = \"python\"):\n",
    "        self.client = ai.Client()\n",
    "        self.model_name = model_name\n",
    "        self.installed_packages = [line.strip() for line in open(\"../requirements.txt\", \"r\").readlines()]\n",
    "        self.system_prompt = f\"\"\"\n",
    "        You are an expert computer vision engineer.\n",
    "        You are given a user prompt and a file type: {file_type}.\n",
    "        The file packages you have access to are as follows: \n",
    "        {self.installed_packages}\n",
    "        You need to generate a code based on the instruction, and the packages you have access to.\n",
    "        The code should be enclosed in <improvement_code> and </improvement_code> tags\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        with open(\"../classification/improvements/improvement_method.py\", \"r\") as f:\n",
    "            self.abstract_example = f.read()\n",
    "\n",
    "        with open(\"../classification/improvements/mixup.py\", \"r\") as f:\n",
    "            self.mixup_example = f.read()\n",
    "    \n",
    "    def generate_code(self, improvement_prompt: str, project_context: str) -> str: \n",
    "        \"\"\"\n",
    "        Generate code on the improvement prompt.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked to write a code to implement the following improvement: {improvement_prompt}.\n",
    "        This code is expected to be used in PyTorch / Lightning collate_fn to process the batch.\n",
    "        The code should be enclosed in <improvement_code> and </improvement_code> tags.\n",
    "        The improvement code should be a class that inherits from the ImprovementMethod class which is as follows:\n",
    "        {self.mixup_example} is an example of how the class should be implemented where it inherits from the ImprovementMethod {self.abstract_example} class.\n",
    "        The project context is as follows:\n",
    "        {project_context}\n",
    "\n",
    "        Please provide:\n",
    "        1. Complete, runnable code with proper imports.\n",
    "        2. Clear documentation and type hints.\n",
    "        3. Integration instructions.\n",
    "\n",
    "        <improvement_code> YOUR CODE HERE </improvement_code>\n",
    "        <integration_instructions> YOUR INTEGRATION INSTRUCTIONS HERE </integration_instructions>\n",
    "        \"\"\"\n",
    "        file_name = improvement_prompt.replace(\" \", \"_\").lower()\n",
    "        path = Path(f\"../classification/improvements/\")\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        results = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "\n",
    "        success, message = self.save_generated_code(results.choices[0].message.content, file_name, path)\n",
    "\n",
    "    def parse_code(self, content: str, tag: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Parse code from XML-like tags with better error handling\n",
    "        \n",
    "        Args:\n",
    "            content: Full LLM response\n",
    "            tag: Tag name (e.g., 'improvement_code', 'integration_instructions')\n",
    "        \n",
    "        Returns:\n",
    "            Extracted code or None if not found\n",
    "        \"\"\"\n",
    "        # Use DOTALL flag to match across newlines\n",
    "        pattern = f\"<{tag}>(.*?)</{tag}>\"\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        # Fallback: try to find markdown code blocks\n",
    "        code_block_pattern = r\"\\n(.*?)\\n```\"\n",
    "        match = re.search(code_block_pattern, content, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def save_generated_code(\n",
    "        self,\n",
    "        response_content: str, \n",
    "        file_name: str, \n",
    "        path: str = \".\",\n",
    "        save_instructions: bool = True\n",
    "    ) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Save generated code and optionally integration instructions\n",
    "        \n",
    "        Returns:\n",
    "            (success, message)\n",
    "        \"\"\"\n",
    "        # Parse code\n",
    "        improvement_code = self.parse_code(response_content, \"improvement_code\")\n",
    "        \n",
    "        if not improvement_code:\n",
    "            return False, \"‚ùå Failed to parse improvement code from response\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the main code file\n",
    "        code_file = Path(path) / f\"{file_name}.py\"\n",
    "        try:\n",
    "            with open(code_file, \"w\") as f:\n",
    "                f.write(improvement_code)\n",
    "            print(f\"‚úÖ Code saved to: {code_file}\")\n",
    "        except Exception as e:\n",
    "            return False, f\"‚ùå Failed to save code: {e}\"\n",
    "        \n",
    "        # Save integration instructions if present\n",
    "        if save_instructions:\n",
    "            integration_instructions = self.parse_code(response_content, \"integration_instructions\")\n",
    "            \n",
    "            if integration_instructions:\n",
    "                instructions_file = Path(path) / f\"{file_name}_INSTRUCTIONS.md\"\n",
    "                try:\n",
    "                    with open(instructions_file, \"w\") as f:\n",
    "                        f.write(integration_instructions)\n",
    "                    print(f\"üìÑ Instructions saved to: {instructions_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Failed to save instructions: {e}\")\n",
    "        \n",
    "        # Also save the raw response for debugging\n",
    "        raw_file = Path(path) / f\"{file_name}.py\"\n",
    "        try:\n",
    "            with open(raw_file, \"w\") as f:\n",
    "                f.write(response_content)\n",
    "            print(f\"üíæ Raw response saved to: {raw_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Failed to save raw response: {e}\")\n",
    "        \n",
    "        return True, f\"Successfully saved {file_name}.py\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd8cc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mixup', 'CutMix', 'RandAugment', 'AutoAugment', 'Canny edge detection', 'Sobel filter concatenation', 'Gabor filter concatenation', 'Histogram equalization', 'Test time augmentation', 'ImageNet pretraining', 'Local binary patterns', 'Laplacian filter concatenation', 'Color jitter', 'Erasing', 'HOG feature concatenation']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import re\n",
    "improvement_string = f\"\"\"\n",
    "<improvement>Mixup</improvement>\n",
    "<improvement>CutMix</improvement>\n",
    "<improvement>RandAugment</improvement>\n",
    "<improvement>AutoAugment</improvement>\n",
    "<improvement>Canny edge detection</improvement>\n",
    "<improvement>Sobel filter concatenation</improvement>\n",
    "<improvement>Gabor filter concatenation</improvement>\n",
    "<improvement>Histogram equalization</improvement>\n",
    "<improvement>Test time augmentation</improvement>\n",
    "<improvement>ImageNet pretraining</improvement>\n",
    "<improvement>Local binary patterns</improvement>\n",
    "<improvement>Laplacian filter concatenation</improvement>\n",
    "<improvement>Color jitter</improvement>\n",
    "<improvement>Erasing</improvement>\n",
    "<improvement>HOG feature concatenation</improvement>\n",
    "\n",
    "\"\"\"\n",
    "improvement_list = re.findall(r'<improvement>(.*?)</improvement>', improvement_string)\n",
    "print(improvement_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb254cbf",
   "metadata": {},
   "source": [
    "# Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ec09393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Code saved to: ../classification/improvements/hog_feature_concatenation.py\n",
      "üíæ Raw response saved to: ../classification/improvements/hog_feature_concatenation.py\n"
     ]
    }
   ],
   "source": [
    "coding_agent = CodeGenerationAgent(model_name=\"ollama:gemini-3-flash-preview\", file_type=\"python\")\n",
    "results = coding_agent.generate_code(improvement_prompt=\"HOG feature concatenation\", project_context=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
