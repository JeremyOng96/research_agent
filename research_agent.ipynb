{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89b57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/research_agent_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import aisuite as ai\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from classification.classification_models.vit import ClassificationModel, ModelConfig\n",
    "from classification.classification_models.vit import ImageWoofDataset\n",
    "from classification.classification_metrics.metrics import ClassificationMetrics\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2da39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research in multimodal object detection has shifted significantly over the last 24 months. We have moved away from \"closed-set\" detection (identifying 80 categories like in COCO) toward **Open-Vocabulary Detection (OVD)** and **Vision-Language (VL) Grounding**, where models can detect any object described in natural language.\n",
      "\n",
      "Here is a breakdown of the latest research trends, key architectures, and paradigms in multimodal object detection as of late 2023 and 2024.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Open-Vocabulary Detection (OVD)\n",
      "The goal of OVD is to detect objects that were not present in the labeled training set by leveraging knowledge from large-scale vision-language models like CLIP.\n",
      "\n",
      "*   **YOLO-World (2024):** This is one of the most significant recent breakthroughs. It introduces a real-time open-vocabulary detector. By using a Vision-Language Path Aggregation Network (RepVL-PAN), it allows the model to detect objects based on custom prompts in real-time, making OVD viable for edge devices.\n",
      "*   **Grounding DINO:** This model combines DINO (a high-performing transformer-based detector) with grounded pre-training. It can perform \"referring expression comprehension,\" meaning you can input \"the blue striped chair next to the window,\" and it will detect that specific object.\n",
      "*   **Detic:** Developed by Meta, it trains classifiers on image-level labels (which are abundant) and applies them to the detection task, significantly expanding the vocabulary size to over 21,000 classes.\n",
      "\n",
      "### 2. Multimodal Large Language Models (MLLMs) as Detectors\n",
      "The hottest area of research is the integration of detection capabilities directly into LLMs (like GPT-4o or LLaVA).\n",
      "\n",
      "*   **Pixel-Level Grounding (LISA, GLaMM):** Research is moving beyond bounding boxes to \"Reasoning Segmentation.\" Models like **LISA (Large Language Instructed Segmentation Assistant)** can take complex queries like \"the fruit that keeps the doctor away\" and output a precise segmentation mask for an apple.\n",
      "*   **Shikra and Kosmos-2:** These are \"Grounding LLMs.\" Unlike standard LLMs that only output text, these models are trained to output coordinates (bounding boxes) as tokens. This allows the model to \"talk\" about specific regions of an image it is looking at.\n",
      "*   **LLaVA-NeXT:** Recent iterations of LLaVA (Large Language-and-Vision Assistant) focus on higher resolution and better spatial reasoning, allowing the model to detect smaller, more nuanced objects in complex scenes.\n",
      "\n",
      "### 3. Unified Multimodal Fusion (Sensor Fusion)\n",
      "In fields like autonomous driving and robotics, multimodal detection involves fusing different sensor modalities (RGB, LiDAR, Thermal, Depth).\n",
      "\n",
      "*   **BEVFusion (Bird's Eye View):** Current research focuses on projecting different modalities (like camera and LiDAR) into a unified Bird's Eye View representation. This prevents information loss during fusion.\n",
      "*   **Language-Prompted 3D Detection:** New research is applying OVD to 3D space. Models are now being trained to detect 3D objects in LiDAR point clouds using natural language prompts, bridging the gap between 2D language models and 3D spatial data.\n",
      "\n",
      "### 4. Efficient Tuning and Adaptation\n",
      "Since training massive multimodal models is expensive, research into **Parameter-Efficient Fine-Tuning (PEFT)** is booming.\n",
      "\n",
      "*   **VL-Adapter:** Researchers are using lightweight adapters (like LoRA) to tune large vision-language models for specific detection tasks without retraining the entire backbone.\n",
      "*   **Prompt Tuning:** Instead of changing the model weights, researchers are \"tuning\" the text prompts or visual prompts to improve detection accuracy in niche domains (e.g., medical imaging or satellite imagery).\n",
      "\n",
      "### 5. Key Challenges & Future Directions\n",
      "The current research frontier is addressing these specific bottlenecks:\n",
      "\n",
      "*   **Small Object Detection:** Multimodal models still struggle with tiny objects because CLIP-based features often lose spatial resolution.\n",
      "*   **Hallucination:** MLLMs sometimes \"detect\" objects that aren't there based on the context of the text prompt (e.g., if you ask for a \"fire extinguisher\" in a kitchen, it might hallucinate one).\n",
      "*   **Inference Speed:** While YOLO-World has helped, most multimodal detectors (especially Transformer-based ones) are still too slow for high-frame-rate robotics applications.\n",
      "\n",
      "### Summary Table: Key Models to Track\n",
      "| Model | Focus | Key Contribution |\n",
      "| :--- | :--- | :--- |\n",
      "| **YOLO-World** | Real-time OVD | High-speed open-vocabulary detection. |\n",
      "| **Grounding DINO** | VL-Grounding | SOTA cross-modal feature fusion for localization. |\n",
      "| **Segment Anything (SAM) + CLIP** | Zero-shot Detection | Using SAM for masks and CLIP for labeling. |\n",
      "| **Kosmos-2** | Embodied AI | Multimodal LLM that treats coordinates as text tokens. |\n",
      "| **BEVFusion** | 3D Detection | Robust fusion of LiDAR and Camera for autonomy. |\n",
      "\n",
      "**Where to follow:** Keep an eye on the **CVPR 2024** and **ECCV 2024** proceedings, as these are the primary venues where the next generation of these models is currently being published.\n"
     ]
    }
   ],
   "source": [
    "client = ai.Client()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can help with computer vision research.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the latest research in computer vision related to multimodal object detection?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ollama:gemini-3-flash-preview\",\n",
    "    messages=messages,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10605266",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32daa58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationErrorAnalysis(pl.LightningModule):\n",
    "    def __init__(self, model:nn.Module, dataloader: DataLoader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "    def get_statistics(self):\n",
    "        # TODO: implement parallel processing for this\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in self.dataloader:\n",
    "            x, y = batch\n",
    "            y_hat = self.model(x)\n",
    "            probs = torch.softmax(y_hat, dim=1)\n",
    "            pred_classes = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_probs.append(probs)\n",
    "            all_preds.append(pred_classes)\n",
    "            all_labels.append(y)\n",
    "        \n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "        self.aucroc = ClassificationMetrics.get_aucroc(all_labels, all_probs)\n",
    "        self.f1 = ClassificationMetrics.get_f1(all_labels, all_preds)\n",
    "        self.accuracy = ClassificationMetrics.get_accuracy(all_labels, all_preds)\n",
    "        self.precision = ClassificationMetrics.get_precision(all_labels, all_preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673a154",
   "metadata": {},
   "source": [
    "# Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe878e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationResearchAgent(ClassificationErrorAnalysis):\n",
    "    def __init__(self, \n",
    "                 user_prompt: str,  \n",
    "                 trained_model: nn.Module,\n",
    "                 dataloader: DataLoader,\n",
    "                 researcher_model_name:\"ollama:gemini-3-flash-preview\"\n",
    "    ):\n",
    "    \n",
    "        super().__init__(trained_model, dataloader)\n",
    "        self.user_prompt = user_prompt\n",
    "        self.ai = ai.Client()\n",
    "        self.system_prompt = f\"\"\"\n",
    "        You are an experienced computer vision practitioner. \n",
    "        You are given a set of statistics and a user prompt. \n",
    "        You need to analyze the statistics and recommend a set of changes to the user prompt to improve the model's performance.\n",
    "        \"\"\"\n",
    "    def analyze_and_recommend(self):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt}\n",
    "        ]\n",
    "        self.get_statistics()\n",
    "        \n",
    "        results = self.ai.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        return results.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b4986",
   "metadata": {},
   "source": [
    "# Research Agent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9202bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationModel(\n",
       "  (model): EfficientNet(\n",
       "    (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNormAct2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(48, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(2, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (aa): Identity()\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(160, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNormAct2d(\n",
       "      1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "trained_model = ClassificationModel.load_from_checkpoint(\n",
    "    \"classification/classification_models/lightning_logs/version_4/checkpoints/tinynet-epoch=10-val_acc=0.8400.ckpt\"\n",
    ")\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1677083",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = Path('/Users/jeremyong/Desktop/research_agent/dataset/imagewoof-160/val')\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "research_agent = ClassificationResearchAgent(\n",
    "    user_prompt=\"What is the latest research in computer vision related to multimodal object detection?\",\n",
    "    trained_model=trained_model,\n",
    "    dataloader=DataLoader(ImageWoofDataset(val_path, transform=val_transform), batch_size=1, shuffle=False),\n",
    "    researcher_model_name=\"ollama:gemini-3-flash-preview\"\n",
    ")\n",
    "\n",
    "recommendation = research_agent.analyze_and_recommend()\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81852ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b3da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
